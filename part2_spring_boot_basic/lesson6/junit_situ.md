# JUnit in Situ

When we write tests, it's with the intention to run them and report on the results. Test runners like JUnit provide many ways to report the results of a test run, but one of the most useful ways to interact with that reporting is through an IDE, like IntelliJ.

There are three main advantages to running JUnit tests from an IDE:

* Interactive Reporting: When we run tests in an IDE, we can usually inspect the results of each test individually. If an assertion fails or an unexpected exception is triggered, the stack trace and circumstances will be shown in the details for each test, and clickable links in the results help you navigate to problem areas in your code.
* Interactive Debugging: When a pernicious problem persists, it can often be helpful to step through the code's execution line-by-line to inspect both the control flow and the values in memory used by the program. This is called debugging, and while it's technically possible to do outside of an IDE, IDEs like IntelliJ provide many useful tools for making the process as painless as possible.
* Code Coverage Reports: When we run code in an IDE like IntelliJ, we can choose to have the IDE track which lines of our code were visited, and how many times. This can be wildly useful when trying to track down why a branch of a condition isn't being reached, as well as when determining how much the entire code base is covered by the currently-implemented tests.

In this first foray into fixing failing tests, we ran our tests within IntelliJ to get a report of the status of all tests. Initially, these were all failing, but by clicking through IntellliJ's test report details, we quickly discovered a common problem to all of the tests: some of the data under test wasn't being initialized at all! We solved this by adding an @BeforeAll-annotated method responsible for that initialization logic. Running the tests again, our report shows that some are slowly turning green - progress! All we needed was a handy overview of the test results, and we could quickly identify a common problem between them.

In this second attempt to fix our failing tests, we used IntelliJ's debugger to check our code under test line-by-line. We found that the conditions our test was validating did not match the code under test - which means we had a decision to make.
Usually, in cases like this, where the test does not match the code it is testing, we have to decide which is correct. In a real-world development scenario, we would check both against the technical requirements provided to us, but since this is just an example, we chose to assume that the code under test was correct.
In any case, debugging helped us find an issue that otherwise might be hard to find. In the next video, we'll try to get the remaining tests passing, and we'll see how code coverage can help us determine if our code is being sufficiently tested.

In this final push to get our tests passing, we looked at the remaining failing tests. The first was another issue of test/code not being in sync: our test expected a specific exception to be thrown, but the code under test wasn't throwing that exception! This is a good example of why assertThrows is useful in a testing context; if our feature requirements or documentation say that a method should throw an exception under certain circumstances, it can cause real problems if it does anything else.
Moving on to the remaining failing test, we saw that it was performing the exact same test as a previous successful test. This is usually a good sign that these tests rely on some data that needs to be initialized identically before each test. Indeed, we found that the data we initialized with an @BeforeAll-annotated method actually needed to be initialized before each test, not all of them, so we changed the @BeforeAll annotation to @BeforeEach.
Finally, to verify that our tests weren't overlooking anything in our code base, we re-ran them with IntelliJ's code coverage feature. This showed us that while our tests were covering nearly all of the lines of code in the project, there was one method we weren't testing at all. After adding a test for that method and re-running the test suite with coverage, we saw our entire codebase lit up in green. Nice!
